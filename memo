[07/01/15 19:24:12] Jun Sugiura: https://docs.google.com/spreadsheets/d/1NMKiZ363-ymYqGn9BlPJWgGpSi39VlB31KG71hstHZw/edit#gid=767761360
[07/01/15 19:40:25] Jun Sugiura: python src/generateTrainingSet.py --input data/dp-test.tuples --quicktest --extkb /work/jun-s/kb --problemno 13 --cat --pathsim1 --pathgroup --insent2 --simwn > tmp.xml
python src/extractXML.py tmp.xml > tmp4viewer.xml
ln -si tmp4viewer.xml ~/work/wsc/local/webint/link-to-result

wsc/local/webint % python ./server.py 8020
[07/01/15 19:50:17] Jun Sugiura: http://brandy:8090/cgi-bin/comeon-new.py?query=16&k=200
[07/01/15 19:59:30] Jun Sugiura: kill `ps aux | fgrep martin | fgrep python | awk '{print $2}'`
[07/01/15 21:14:09] Jun Sugiura: -- copy the results for all problems in test set.
scp /home/jun-s/work/wsc/tmp.0106full.xml .

-- make symbolic link to the xml file
ln -si  ~/nlp/wsc/tmp.0106full.xml ~/nlp/wsc/local/webint/link-to-result
[07/01/15 21:16:35] Jun Sugiura: -- then, you can use (if you set port number to "8091")
https://docs.google.com/spreadsheets/d/1i1nBydBFfXoY13ba3nXovlRUiWMTl3R12RrpBuLLyLY/edit?usp=sharing

---------------------

Generic sentence (GS):
DO NOT express specific episode(sent1) or isolated facts(sent2).
sent1: “Merry is smoking a cigar.”
sent2: “The bomb caused chaos.”
DO report a kind of general property(sent3) or habit(sent4).
sent3: “potatoes contain protein.”
sent4: “Merry smokes cigar after dinner.”
Criteria of annotation:
1)according to the definition and your understanding of the sentence.
2)if the sentence included frequency adverb like “usually”, “typically”, or temporal modifier like “every night”, or modal verbs like “could”, “would”, “can”, the sentence would be annotated as generic sentence(g).
3)you could test the sentence by adding “usually” before the predicate, and if its meaning doesn’t change a lot, it could be generic sentence.


--------------------

scp command for kth latex compiler: scp -r * martinp4@u-shell.csc.kth.se:~/ex/

--------------------
march wed 11: I started watching the stanford course by jurafsky instead of the columbia one with collins. Much better. I think it will take about two weeks to finish this course. While I'm watching I will attempt to write down thoughts related to the project and suggest improvements along the way, rather than looking at individual sentences from the result set.

So I guess the idea is to work with this for around two weeks and try to create some sort of list with suggestions. At this moment they're kind of vague, but it's better than nothing at all.

* edit distance, filtering on candidates, spell checking on scripts, implement probablistic models, make use of stuff such as google n-gram, try different corpora (newspaper vs fiction?) and compare the results. (see notebook)

Oh yeah, and I reaaallly need to write that project specification.
------------------
march sat 14:
good turing smoothing: given some words, count the number of words and then predict how likely it is that a word will occur in a sentence. smoothing is basically about solving the problem of zero-probabilities by taking probability mass from a common word to a word that never occurs. (isn't it possible to solve the zero anaphora problem with this line of thought?)

------------------
march, mon 16
I just watched videos of chatbots (like cleverbot and siri) talking to eachother. This actually gave me some very interesting insights. It feels like we are creating AI and making language processing in order to make it easier for humans to communicate with eachother and for human-machine communication. However, machine to machine-communication seems so profound and interesting, shouldn't we spend more time with this?

----------------
march, tue 17
Remember to look up stuff regarding sentiment polarity, freebase, wikipedia stuffz. I have learned about sentiment analysis, text classification and some algorithms related to that (naive bayes, bag of words). We could perhaps use classification algorithms on the input questions and map them to classified parts of the corpus (perhaps we're already doing this?).

Regarding sentiment analysis:
It is important to consider negation. There is a resource called "SentiWordNet", can we use it? There are a lot of "polarity lexicons" such as mpqa, opinion lexicon, general inquirer, sentiwordned and liwc.

You can use some classifier graph similarity thing for sentiment polarity (hatzivassiloglou, mckeown, 1997). This has something to do with PMI. 

Polarity(phrase) = PMI(phrase, "excellent") - PMI(phrase, "poor")

Another paper on finding sentiment for aspects (2008 paper, Blair-Goldensohn et al)

to sum it up:
* negation is important
* naive bayes, for some tasks it will work to use all tasks
* use hand-built polarity lexicons (for sentiment polarity)
* use seeds and semi-supervised learning to generate lexicons

Affective states are similar to sentiment, but not really the same thing. I feel that sentiment analysis is more about telling the robot to make a distinction if a text is describing something good or something bad. The study of affective states is much broader. Here we are interested in: emotion, mood, interpersonal states, attitudes and/or personality traits.

There are a lot of creative ways to attack these problems (see 7-5, stanford course). Stanford currently does research on friendliness. Like in what way do we express friendliness and in what situations does it occur? Am I more likely to be friendly towards different persons in different environments or is it something else that decides this?

march, wed 18
------------------
I've been adding some sections to the project specification.

march 24
------------------
Been watching videos and doing some reading on deep learning, some sort of buzzword. There's a model made by stanford that uses deep learning that solves sentiment polarity with good results. Perhaps this is something to look at? Deep learning is based on using old learning algorithms combined with the use of big data. The method is simple but produces complex results (similar to mandelbrot fractals ~ish).

Sentiment Analysis Treebank Stanford stuff: http://nlp.stanford.edu/~socherr/EMNLP2013_RNTN.pdf , http://nlp.stanford.edu/sentiment/
