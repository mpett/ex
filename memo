07/01/15 19:24:12] Jun Sugiura: https://docs.google.com/spreadsheets/d/1NMKiZ363-ymYqGn9BlPJWgGpSi39VlB31KG71hstHZw/edit#gid=767761360
[07/01/15 19:40:25] Jun Sugiura: python src/generateTrainingSet.py --input data/dp-test.tuples --quicktest --extkb /work/jun-s/kb --problemno 13 --cat --pathsim1 --pathgroup --insent2 --simwn > tmp.xml
python src/extractXML.py tmp.xml > tmp4viewer.xml
ln -si tmp4viewer.xml ~/work/wsc/local/webint/link-to-result

wsc/local/webint % python ./server.py 8020
[07/01/15 19:50:17] Jun Sugiura: http://brandy:8090/cgi-bin/comeon-new.py?query=16&k=200
[07/01/15 19:59:30] Jun Sugiura: kill `ps aux | fgrep martin | fgrep python | awk '{print $2}'`
[07/01/15 21:14:09] Jun Sugiura: -- copy the results for all problems in test set.
scp /home/jun-s/work/wsc/tmp.0106full.xml .

-- make symbolic link to the xml file
ln -si  ~/nlp/wsc/tmp.0106full.xml ~/nlp/wsc/local/webint/link-to-result
[07/01/15 21:16:35] Jun Sugiura: -- then, you can use (if you set port number to "8091")
https://docs.google.com/spreadsheets/d/1i1nBydBFfXoY13ba3nXovlRUiWMTl3R12RrpBuLLyLY/edit?usp=sharing

---------------------

Generic sentence (GS):
DO NOT express specific episode(sent1) or isolated facts(sent2).
sent1: “Merry is smoking a cigar.”
sent2: “The bomb caused chaos.”
DO report a kind of general property(sent3) or habit(sent4).
sent3: “potatoes contain protein.”
sent4: “Merry smokes cigar after dinner.”
Criteria of annotation:
1)according to the definition and your understanding of the sentence.
2)if the sentence included frequency adverb like “usually”, “typically”, or temporal modifier like “every night”, or modal verbs like “could”, “would”, “can”, the sentence would be annotated as generic sentence(g).
3)you could test the sentence by adding “usually” before the predicate, and if its meaning doesn’t change a lot, it could be generic sentence.


--------------------

scp command for kth latex compiler: scp -r * martinp4@u-shell.csc.kth.se:~/ex/

--------------------
march wed 11: I started watching the stanford course by jurafsky instead of the columbia one with collins. Much better. I think it will take about two weeks to finish this course. While I'm watching I will attempt to write down thoughts related to the project and suggest improvements along the way, rather than looking at individual sentences from the result set.

So I guess the idea is to work with this for around two weeks and try to create some sort of list with suggestions. At this moment they're kind of vague, but it's better than nothing at all.

* edit distance, filtering on candidates, spell checking on scripts, implement probablistic models, make use of stuff such as google n-gram, try different corpora (newspaper vs fiction?) and compare the results. (see notebook)

Oh yeah, and I reaaallly need to write that project specification.
------------------
march sat 14:
good turing smoothing: given some words, count the number of words and then predict how likely it is that a word will occur in a sentence. smoothing is basically about solving the problem of zero-probabilities by taking probability mass from a common word to a word that never occurs. (isn't it possible to solve the zero anaphora problem with this line of thought?)

------------------
march, mon 16
I just watched videos of chatbots (like cleverbot and siri) talking to eachother. This actually gave me some very interesting insights. It feels like we are creating AI and making language processing in order to make it easier for humans to communicate with eachother and for human-machine communication. However, machine to machine-communication seems so profound and interesting, shouldn't we spend more time with this?

----------------
march, tue 17
Remember to look up stuff regarding sentiment polarity, freebase, wikipedia stuffz. I have learned about sentiment analysis, text classification and some algorithms related to that (naive bayes, bag of words). We could perhaps use classification algorithms on the input questions and map them to classified parts of the corpus (perhaps we're already doing this?).

Regarding sentiment analysis:
It is important to consider negation. There is a resource called "SentiWordNet", can we use it? There are a lot of "polarity lexicons" such as mpqa, opinion lexicon, general inquirer, sentiwordned and liwc.

You can use some classifier graph similarity thing for sentiment polarity (hatzivassiloglou, mckeown, 1997). This has something to do with PMI. 

Polarity(phrase) = PMI(phrase, "excellent") - PMI(phrase, "poor")

Another paper on finding sentiment for aspects (2008 paper, Blair-Goldensohn et al)

to sum it up:
* negation is important
* naive bayes, for some tasks it will work to use all tasks
* use hand-built polarity lexicons (for sentiment polarity)
* use seeds and semi-supervised learning to generate lexicons

Affective states are similar to sentiment, but not really the same thing. I feel that sentiment analysis is more about telling the robot to make a distinction if a text is describing something good or something bad. The study of affective states is much broader. Here we are interested in: emotion, mood, interpersonal states, attitudes and/or personality traits.

There are a lot of creative ways to attack these problems (see 7-5, stanford course). Stanford currently does research on friendliness. Like in what way do we express friendliness and in what situations does it occur? Am I more likely to be friendly towards different persons in different environments or is it something else that decides this?

march, wed 18
------------------
I've been adding some sections to the project specification.

march 24
------------------
Been watching videos and doing some reading on deep learning, some sort of buzzword. There's a model made by stanford that uses deep learning that solves sentiment polarity with good results. Perhaps this is something to look at? Deep learning is based on using old learning algorithms combined with the use of big data. The method is simple but produces complex results (similar to mandelbrot fractals ~ish).

Sentiment Analysis Treebank Stanford stuff: http://nlp.stanford.edu/~socherr/EMNLP2013_RNTN.pdf , http://nlp.stanford.edu/sentiment/

march 24, cont'd
-----------------
Watched a video with a researcher from BaiDu talking about hpc in machine learning and the use of GPU:s in solving ML problems. Continued reading the paper on semantic treebank whatever things. Read about semantic vector spaces.

march 25
-----------------
Watched a video on deep learning with Python, pretty boring tbh.

march 26
-----------------
I read the paper on sentiment treebanks again. I solved a programming exercise.

march 27
----------------
Watched a video on deep learning by Andrew Ng from three months ago. Looked at the documentation for stanford corenlp.

march 30
----------------
Practiced Japanese and wrote some letters for KTH.

april 1
----------------
Listened to programming podcast, did some more paperwork for KTH, practiced Japanese.

may 8
----------------
Re-reading Levesque paper on the WS challenge in order to help me get back on track. 

Right now my thoughts are like this:

A research question for my project might be: "Can the use of sentiment polarity be beneficial for a known coreference resolution model based on script knowledge?"

I would like to add the sentiment treebank thing to the current system and see if it improves it. Would be an idea to define several improvements based on sentiment polarity and make a big comparison in the end. I'm still not sure but will try to compile a list of possible improvements/experiments by Thursday so I have it before the KIAI meeting.

May 19
------------------
Had a discussion with Inoue-san about sentiment polarity and selectional preference. Will now try to explore the idea of combining sentiment treebanks from the Stanford paper with existing models in selectional preference.

May 21
-----------------
Talked with Inui-san. I should come to the lab more often. Ballsballsballs.

May 22
-----------------
Read paper on RNTN again. Learned about numerical analysis (secret reason). Attended KIAI.

May 25
-----------------
Started reading papers on selectional preference.

May 26
-----------------
Same as yesterday

May 27
-----------------
Meeting with Inoue-san. Talked about SP and what models we should use.

June 1
-----------------
Attended KIAI last Friday. I should move my focus to implementing different models of Selectional Preference. Need to figure out how to do that. I also really need to get bureaucratic stuff done. Also attended a nice welcome dinner.

June 4
-----------------
Attended research meeting. Managed to extract word pairs from the clueweb12 resource. Still wondering how to include polarity for each word pair, this needs some extra thought. One way would be to use the Wilson resource.

I managed to extract around 200k wordpairs from clueweb, but the Wilson dictionary only has around 8k entries. Clearly this is not enough. One approach is to annotate the extracted pairs for which there are entries for and build a classifier using these as training data.

I'm still not sure about that, it seems a bit too crude.

June 5
------------------
In Tokyo. Reading Anders Westling's thesis on sentiment analysis for twitter posts with machine learning.

June 10
-----------------
Today: Looked at the testbed sentences Inoue-san sent me. I now have 4 sentiment lexicons along with clueweb.
Tomorrow: Perform error analysis on the testbed sentences, attend research meetings and figure out what the hell I'm supposed to do.

June 11
-----------------
I looked at the testbed and got some sort of explanation of why the sentiment preference model might help or at least improve the situation.

Today: Extract sentiment values for the direct object pair list, make some sort of naive counter model and make slides for tomorrow's meeting.
Tomorrow: Attend KIAI.

June 12
-----------------
I attended KIAI. Didn't understand anything. I should really, really make slides for next time.
Today: Drink beer.
Tomorrow: Do what I should have done yesterday.

June 14
-----------------
Sentiment values extracted, but there are some bugs.
Todo: Fix bugs, create counter function, do experiment on testbed sentences, make slides.

Before June 17
-----------------
Sentiment value extraction now working properly.
Todo: preference calculator, perform experiment, make slides for Friday's KIAI.

June 19
-----------------
Preference calculator completed. Made presentation at KIAI.
Todo: Model evaluation.

June 30
-----------------
Finished model evaluation on naive sp. Seems pretty worthless?
